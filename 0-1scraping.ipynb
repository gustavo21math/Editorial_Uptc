{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Scraper` de `Datos Editorial UPTC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Scraper` de `URLs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url de libros Uptc\n",
    "url = 'https://editorial.uptc.edu.co/catalogo.html?product_list_limit=60'\n",
    "bucle = 'https://editorial.uptc.edu.co/catalogo.html?p=2&product_list_limit=60'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html')\n",
    "\n",
    "\n",
    "scrap = soup.find('div',class_=\"sa-product-container-v3 grid\")\n",
    "productos_url = scrap.find_all('li',class_=\"sa-product-item-v3 sa-product_grid_size\")\n",
    "url_libros = [url.find('a')[\"href\"] for url in productos_url]\n",
    "\n",
    "urls = pd.DataFrame(url_libros,columns=['url'])\n",
    "\n",
    "for i in range(2,6):\n",
    "    url = f'https://editorial.uptc.edu.co/catalogo.html?p={i}&product_list_limit=60'\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text,'html')\n",
    "    productos_url = soup.find('div',class_=\"sa-product-container-v3 grid\").find_all('li',class_=\"sa-product-item-v3 sa-product_grid_size\")\n",
    "    url_libros = [url.find('a')[\"href\"] for url in productos_url]\n",
    "    urls = pd.concat([urls,pd.DataFrame(url_libros,columns=['url'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls.to_json('urls.json',index=False)\n",
    "urls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_libros = pd.read_csv('urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_libros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_palabras_clave(texto):\n",
    "    # Patrón para encontrar palabras clave y sus valores\n",
    "    patron = r'([^:]+):\\s*([^\\n]+)'\n",
    "    # Encontrar todas las coincidencias\n",
    "    coincidencias = re.findall(patron, texto)\n",
    "    \n",
    "    # Crear diccionario con las palabras clave\n",
    "    palabras_clave = {}\n",
    "    for clave, valor in coincidencias:\n",
    "        # Limpiar espacios en blanco\n",
    "        clave = clave.strip()\n",
    "        valor = valor.strip()\n",
    "        palabras_clave[clave] = valor\n",
    "    \n",
    "    return palabras_clave\n",
    "\n",
    "def extraer_info_colaborador(bloque):\n",
    "    # Nombre invertido\n",
    "    nombre_invertido = bloque.split('\\n')[0].strip()\n",
    "    \n",
    "    # ORCID\n",
    "    orcid_match = re.search(r'(https://orcid\\.org/[^\\s]+)', bloque)\n",
    "    orcid = orcid_match.group(1) if orcid_match else None\n",
    "    \n",
    "    # Afiliación\n",
    "    afiliacion_match = re.search(r'Afiliación\\s*\\n([^\\n]+)', bloque)\n",
    "    afiliacion = afiliacion_match.group(1).strip() if afiliacion_match else None\n",
    "    \n",
    "    # Biografía\n",
    "    bio_match = re.search(r'Biografía:\\s*([\\s\\S]+?)(?:\\n\\S|$)', bloque)\n",
    "    if bio_match:\n",
    "        biografia = bio_match.group(1).strip()\n",
    "    else:\n",
    "        # Si no encuentra el patrón, toma todo después de \"Biografía:\"\n",
    "        bio_idx = bloque.find(\"Biografía:\")\n",
    "        biografia = bloque[bio_idx+10:].strip() if bio_idx != -1 else None\n",
    "    \n",
    "    # Email (opcional)\n",
    "    email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', bloque)\n",
    "    email = email_match.group(0) if email_match else None\n",
    "\n",
    "    return {\n",
    "        \"nombre_invertido\": nombre_invertido,\n",
    "        \"orcid\": orcid,\n",
    "        \"afiliacion\": afiliacion,\n",
    "        \"biografia\": biografia,\n",
    "        \"email\": email\n",
    "    }\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_datos = []\n",
    "for i in range(144,298):\n",
    "    titulos = []\n",
    "    autores = []\n",
    "    palabras_clave = []\n",
    "    precios = []\n",
    "    descripcion = []\n",
    "\n",
    "\n",
    "    res = requests.get(url_libros['url'][i])\n",
    "    soup = BeautifulSoup(res.text,'html')\n",
    "    time.sleep(5)\n",
    "    scrap=soup.find('div',class_=\"product-info-main\")\n",
    "    try:\n",
    "        titulos.append(scrap.find('div',class_=\"col-xs-12\").find('h2').text)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    ## busquemos lo autores\n",
    "    temp = [i for i in scrap.find_all('div',class_=\"col-xs-12\")]\n",
    "    temp1 =[ a.text for a in temp[1].find_all('a')  if 'products_author' in a['href'] ]\n",
    "    autores.append(temp1)\n",
    "    # busca las palabras clave\n",
    "    temp1 =[ a.text for a in temp[1].find_all('a')  if 'products_keywords' in a['href'] ]\n",
    "    palabras_clave.append(temp1)\n",
    "    # DOi\n",
    "    try:\n",
    "        temp1 =[ a.text for a in temp[1].find_all('a')  if 'products_keywords' not in a['href'] and  'products_author' not in a['href']]\n",
    "        if temp1 != []:\n",
    "            doi = temp1\n",
    "        else :\n",
    "            doi = None \n",
    "    except Exception as e:\n",
    "        print(f'No se encontro doi: {e}')\n",
    "        doi = None\n",
    "\n",
    "    # precios\n",
    "    try:\n",
    "        temp = [a for a in scrap.find_all('div',class_='price-box price-final_price')]\n",
    "        if temp != \"\":  # Si hay elementos de precio\n",
    "            temp1 = [a.text.strip() for a in temp]\n",
    "            precios.append(temp1)\n",
    "        else:  # Si no hay elementos de precio\n",
    "            precios.append(['Descargable gratis'])\n",
    "    except Exception as e:\n",
    "        print(f'No se encontro precio: {e}')\n",
    "        precios.append(['Descargable gratis'])\n",
    "    # descripcion\n",
    "    descripcion.append(scrap.find('div',class_='description-wrapper').text)\n",
    "    # eBook\n",
    "    try:\n",
    "        scrap = soup.find('div',class_=\"product-info-metadata\")\n",
    "        metadata_ebook = extraer_palabras_clave(scrap.text)\n",
    "    except Exception as e:\n",
    "        print(f'No se encontro metadata_ebook: {e}')\n",
    "        metadata_ebook = None\n",
    "    #impreso\n",
    "    try:\n",
    "        scrap = soup.find_all('div',class_=\"product-info-metadata\")[1]\n",
    "        metadata_imp = extraer_palabras_clave(scrap.text)\n",
    "    except Exception as e:\n",
    "        print(f'No se encontro metadata_imp: {e}')\n",
    "        metadata_imp = None\n",
    "    #clasificacion de tematica\n",
    "    dato = soup.find('div','item content').text.replace('\\n','')\n",
    "    clasificacion_tematica = dato\n",
    "    #tabla de contenido\n",
    "    a=soup.find('div',class_='product data items').text.find('Tabla de contenido')\n",
    "    b=soup.find('div',class_='product data items').text.find('Colaboradores')\n",
    "    dato = soup.find('div',class_='product data items').text\n",
    "    if a != -1:\n",
    "        tabla_contenido=dato[a:b]\n",
    "    else:\n",
    "        tabla_contenido= None\n",
    "    ## informacion de autores\n",
    "    a=soup.find('div',class_='product data items').text.find('Colaboradores')\n",
    "    dato = soup.find('div',class_='product data items').text\n",
    "    colaboradores_raw = dato[a:].split('Nombre invertido')[1:]\n",
    "    colaboradores = []\n",
    "    for bloque in colaboradores_raw:\n",
    "        info = extraer_info_colaborador(bloque)\n",
    "        colaboradores.append(info)\n",
    "\n",
    "    datos = {\n",
    "            \"title\": titulos,\n",
    "            \"colaboradores\":autores,\n",
    "            \"precio\":precios,\n",
    "            \"palabras clave\":palabras_clave,\n",
    "            \"doi\":doi,\n",
    "            \"descripcion\":descripcion,\n",
    "            \"metadata_ebook\":metadata_ebook,\n",
    "            \"metadata_imp\":metadata_imp,\n",
    "            \"clasificación_temática\":clasificacion_tematica,\n",
    "            'tabla_contenido':tabla_contenido,  \n",
    "            'inf_autores' :colaboradores \n",
    "            }\n",
    "    temp_datos.append(datos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 1. Leer los datos existentes (si el archivo existe)\n",
    "if os.path.exists('datos.json'):\n",
    "    with open('datos.json', mode='r', encoding='utf-8') as f:\n",
    "        datos_existentes = json.load(f)\n",
    "else:\n",
    "    datos_existentes = []\n",
    "\n",
    "# 2. Agregar los nuevos datos (supongamos que tus nuevos datos están en temp_datos)\n",
    "datos_existentes.extend(temp_tatos1)\n",
    "\n",
    "# 3. Guardar la lista completa\n",
    "with open('datos.json', mode='w', encoding='utf-8') as f:\n",
    "    json.dump(datos_existentes, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://editorial.uptc.edu.co/gpd-el-ano-viejo-9790900532909.html'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url_libros['url'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datos_existentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tatos1.extend(temp_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
